title,jd,proposal,status,hired,proposals
Zapier & API Expert for multiple softwares integration,"🌟 About Us:
We are a small but fast growing and leading residential property management company known for our commitment to excellence and innovation. We're on a mission to enhance our operations through efficient task management and automation, specially on Zapier.

💼 Job Role:
As a Zapier Expert, you will play a pivotal role in revolutionizing our workflow management system. Your primary responsibility will be of:

🔸 Designing and implementing efficient task automation solutions between:

1. Our Property Management software, Buildium, having an « open API » and;
- Asana (Trigger when maintenance task create in Buildium, it will send it to Asana task to work on these tasks from Asana workflow that is already set)
(Multiple other triggers, from both apps, sync)
- Jotform (only if possible to create an owner bill in Buildium when staff fill out a Jotform)
- Xero (only if possible to send all the accounting numbers into Xero, to make Xero our primary accounting software)

2. Our short-term rental management software Hostaway and;
- Xero (if possible, to send all the accounting numbers to Xero)

3. Calendly & Asana (trigger Calendly when have new meetings and send all the details to Asana, in order for our showing agents to give an update/feedback for each prospect in Asana)

4. Amazon.ca and Xero (all purchases accounting numbers would go into Xero)

5. Etc…

Note: Please don’t use Zapier if one of the software already have an integration with the other software. Unless more efficient.

Thank you and wishing you an amazing week!!","I am an expert Automation Engineer with more than 5 years of experience and have automated business workflows for dozens of clients through Zapier (and Make) and API integrations. You can verify my expertise by checking a few of my relevant past projects at the following links.

https://www.upwork.com/freelancers/usmanashraf678?p=1712450941576347648
https://www.upwork.com/freelancers/usmanashraf678?p=1712449672145350656
https://www.upwork.com/freelancers/usmanashraf678?p=1704767250568237056

In addition to these, I have also automated several workflows through API integrations (by using Python scripts) including Amazon, Xero, and Asana. You can find more details about them below.

https://www.upwork.com/freelancers/usmanashraf678?p=1594937832742051840
https://www.upwork.com/fl/usmanashraf678?p=1693976404040691712
https://www.upwork.com/fl/usmanashraf678?p=1594943344174948352

Let's connect for a quick session to discuss the specifics and I would love to offer a FREE plan of action for the job.

Many Thanks,",Viewed,2,10
"Athena EHR customization; data analytics and visualization using Power BI, Tableau, or Looker","Concierge Medical Practice looking for Athena EHR expert to customize Athena fields and allow for export of patient health data for analysis and visualization of health metrics to present to patients.

Not sure if Tableau, Power BI, or Looker would be the best fit, but we'd like to be able to use the model you develop in the future by having the ability to add new data fields when required and showcase the any data we select to our patients in a dashboard-type layout with organized/appealing graphics. Each patient will have a different focus for their health, so the data that will be presented to one will be different from the next, and so on.

Open to a short call to discuss more if need be. ","I understand that you are looking for effective ways to visualize your data coming from Athena EHR. I would recommend using Tableau for this kind of data.

I suggest creating a few custom dashboards and have these different dashboards populate for the patient based on their heath focus. Here are a couple of dashboarding projects that I have completed:

Marketing Data Dashboard: https://www.upwork.com/freelancers/usmanashraf678?p=1704767250568237056
Facebook Ads Reporting Dashboard: https://www.upwork.com/fl/usmanashraf678?p=1595719039010119680

PS. I have a Masters in Bioengineering from Georgia Tech so I have the relevant background knowledge about EHRs that would help me make this project a success!

Let's connect and I'd love to share a detailed plan on how to execute this project!",Viewed,0,50
Azure Integrations Data Engineer,"Job Overview: We are seeking a skilled Azure Data Factory Integration Engineer to join our team. The ideal candidate will have experience in designing, developing, and maintaining ETL pipelines using Azure Data Factory. Additionally, familiarity with SFTP connections, handling various flat file formats such as CSV, tab delimited files, and XML is essential for this role. As part of our team, you will be responsible for integrating data from diverse sources, ensuring data quality, and optimizing performance within Azure Data Factory.

Responsibilities:

Design, develop, and implement ETL pipelines using Azure Data Factory to extract, transform, and load data from various sources into target destinations.

Collaborate with stakeholders to gather requirements and understand data integration needs.

Configure and manage connections to SFTP servers for data ingestion and extraction processes.

Handle diverse flat file formats including CSV, tab delimited files, and XML within Azure Data Factory pipelines.

Implement data validation and quality checks to ensure accuracy and completeness of the integrated data.

Optimize performance of ETL processes to meet performance and scalability requirements.

Monitor, troubleshoot, and resolve issues related to data integration processes.

Document technical specifications, workflows, and procedures related to Azure Data Factory pipelines.

Stay updated with the latest Azure Data Factory features and best practices, and evaluate their applicability to our data integration solutions.

Collaborate with other team members including data engineers, data analysts, and business stakeholders to deliver successful data integration solutions.

Requirements:

Bachelor’s degree in Computer Science, Information Technology, or related field.

Proven experience working with Azure Data Factory for ETL processes.

Proficient in T-SQL, P-SQL or Azure SQL

Proficiency in configuring and managing SFTP connections within Azure Data Factory.

Strong understanding of flat file formats such as CSV, tab delimited files, and XML.

Experience with data validation techniques and ensuring data quality within ETL pipelines.

Familiarity with performance optimization techniques for ETL processes.

Ability to troubleshoot and resolve issues related to data integration processes.

Excellent communication skills and the ability to collaborate effectively with cross-functional teams.

Detail-oriented with strong analytical and problem-solving skills.

Azure certifications related to data engineering or Azure Data Factory are a plus.

Preferred Qualifications:

Experience with other Azure services such as Azure SQL Database, Azure Data Lake Storage, or Azure Blob Storage.

Familiar with Restful API’s","I am an expert Azure Data Engineer with 5+ years of exprience and have developed several ETL pipelines using Azure Data Factory, Data Lake, Blob Storage, and SQL while ensuring efficient data extraction, transformation, and loading from various sources into target destinations. I have created dozens of pipelines and scripts using Rest APIs and you can verify that in some of the past projects shared below. I am also skilled in configuring SFTP connections, handling CSV, XML, and other flat file formats, and implementing data validation and quality checks to preserve data integrity at all times.

https://www.upwork.com/freelancers/usmanashraf678?p=1757640981355171840
https://www.upwork.com/freelancers/usmanashraf678?p=1594587629741510656

I am eager to share how I can make a positive impact on your project. Let's connect for a quick chat and I would love to discuss the details and show how I can use my expertise to make your data integration process optimized and scalable.

Many Thanks,",Ignored,1,50
AI Transformation Project,"We are looking for an experienced resource who can develop an AI code to do the following:

1- Understand certain business, educational, shape images from google and transform it into an editable format of PPT , PSD or JSON

2- Provide a testable demo to test the code

3- Assist the current developers in implementing the code into the platform

4- Arabic and english language proficiency is preferable","Let's think out side the box! I would recommend Open AI for this AI Transformation project. I have used Open AI vision model to extract data from images. Let's connect and discuss the possibility of doing this for business, educational, shape images from google. Putting the data into editable format of PPT , PSD or JSON after we have identified it using AI would not be any difficult. I can provide a testable demo for a small budget.

Let's connect and discuss this!

Usman",Ignored,0,50
Data Pipeline Engineer - Snapchat API to Google BigQuery Integration,"We are a video company with channels on Snapchat. We pull performance data from Snapchat and store it in Big Query databases. We are seeking an experienced Data Pipeline Engineer to fix and optimize our existing data pipeline between Snapchat Video API and Google BigQuery. The ideal candidate will have a strong understanding of data pipelines, AWS, Google BigQuery, and the Snapchat API. The primary responsibility will be to ensure the smooth flow of data from Snapchat to Google BigQuery while optimizing API calls for efficiency and cost management.

Responsibilities:

Identify and resolve any issues in the existing data pipeline between Snapchat's SnapKit API and Google BigQuery.
Optimize API calls to minimize cost and improve performance.
Develop and maintain data transformation processes to ensure data integrity and consistency.
Monitor and troubleshoot pipeline performance, ensuring reliability and scalability.
Document pipeline architecture, processes, and configurations for future reference.

Requirements:

- Strong understanding of data pipelines and ETL processes.
- Proficiency in AWS services such as S3, Lambda, and EC2.
- Experience working with the Snapchat API and familiarity with its authentication and data retrieval mechanisms.
- In-depth knowledge of Google BigQuery, including SQL querying and schema design.
- Ability to optimize API calls for performance and efficiency.
- Excellent problem-solving skills and attention to detail.
- Strong communication and collaboration skills to work effectively with cross-functional teams.
- Prior experience in optimizing data pipelines for large-scale data processing is preferred.
- Bachelor's degree in Computer Science, Engineering, or related field is a plus.

How to Apply:
If you meet the above requirements and are excited about the opportunity to optimize data pipelines between the Snapchat API and Google BigQuery, please submit your proposal detailing your relevant experience and your approach to solving the challenges outlined. Include any relevant samples of previous work or projects that demonstrate your expertise in data pipeline optimization. ","I am a GCP certified Data Pipeline Engineer with half a decade of experience in integrating social media APIs such as Snapchat API to Google BigQuery. I have created data pipelines in AWS (S3, Lambda, and EC2.) and Google BigQuery. For creating your ETL processes, I am curious what issues you're facing in your current pipeline exactly (other than query optimization that you mentioned).

Some of my projects documented:
Instagram API + AWS + Snowflake: https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656
Delivery Platform API + Big query: https://www.upwork.com/fl/usmanashraf678?p=1693978841274896384

Let's discuss and I'd love to analyze and optimize your current pipeline. I have created 100s of such pipelines previously.

Many Thanks,
",Ignored,0,50
Custom MS Copilot Development,"Objective: To create co-pilot that can query the data and responds to the user questions in an accurate and reproducible manner. We are seeking a prompt engineer freelancer to undertake a short-term project focused on creating custom copilots. The primary objective is to develop a custom copilot capable of querying data from an Excel file, the sample of which is provided. This copilot will be utilized to efficiently handle data queries, searches, and aggregations based on background data.

The bot should be able to query data in excel/data verse which will have atleast ~200K records.

The excel file has following columns SKU Name, Material Number, Font Format, Foundry Name, Family Name and few more. The bot should be able to take the font name provided by the user and return all the matching records along with information in other defined columns

The bot should be able to search on partial/full font names and also on the font format column

The bot should first return the output in an tabular format and then return the list of material numbers in a comma separated format once user confirms or asks for it.

The bot should be able to answer all questions correctly as attached.

Acceptance Criteria:
The successful completion of this task will be evaluated based on the following criteria:
Accuracy: The copilot's responses to user queries must be accurate.
Consistency: The copilot should provide consistent answers over time and across different users.
Specificity: Generic responses to specific questions are unacceptable; the copilot must provide tailored and relevant answers.","I am an experienced prompt engineer and have already worked on a similar project where I developed a custom copilot for querying databases based on natural language commands. You can check more details about that project at https://www.upwork.com/fl/usmanashraf678?p=1693960848319623168

In addition to that, I have implemented dozens of automation scripts to search and aggregate required data from huge Excel and CSV files. You can check a few of these projects at the following links.

https://www.upwork.com/fl/usmanashraf678?p=1693977732631453696
https://www.upwork.com/freelancers/usmanashraf678?p=1693952886184968192
https://www.upwork.com/freelancers/usmanashraf678?p=1693973712367071232

Based on that, the copilot will be programmed to answer the questions and tailor relevant answers for each question (and user). The sample file seems to be missing and it would be easier to quote the exact price once I had a look at it. For now, I have mentioned a tentative price and time frame.

Let's connect for a quick chat to discuss the specifics and I would love to offer more insights and a detailed SOW (for FREE).

Many Thanks,
",Ignored,2,50
Artificial Intelligence Tutor,"Hi, we are looking for some engineers who have deep learning and artificial intelligence project experience.

Please show me some of your past AI project, and during the job you will guide the intern to complete that project again. It doesn't have to be a big project. Even school coding project fore artificial intelligence is sufficient.

Overall the job should be easy if you have some projects in mind.

Thank you.","I have taught ML and AI to high school students at America's top Tech Camp called iD Tech. I have recently completed a number AI projects which I am highlighting with this job as well. I'd love to train and tutor you on these Artificial Intelligence projects.

I love teaching. So, let's connect and discuss!

Many Thanks,",Ignored,1,50
Zapier Automation Expert,"We are seeking a Zapier Automation Expert to streamline our workflow and automate repetitive tasks. The ideal candidate should be proficient in Zapier and have experience with various integrations. The job involves designing and implementing automation workflows, troubleshooting issues, and optimizing processes. The successful candidate will have excellent problem-solving skills, attention to detail, and the ability to work independently.

Relevant skills:
- Proficient in Zapier
- Strong understanding of various integrations
- Experience with workflow automation
- Problem-solving skills
- Attention to detail
- Ability to work independently","I am an experienced Automation Expert who has implemented dozens of integrations and workflow automations with Zapier, Make, and other tools. You can verify my skills by checking more details about some of my past projects.

https://www.upwork.com/freelancers/usmanashraf678?p=1712450941576347648
https://www.upwork.com/freelancers/usmanashraf678?p=1712449672145350656
https://www.upwork.com/freelancers/usmanashraf678?p=1704767250568237056

Let's connect for a quick session to discuss the specifics of the tasks and I would love to share a FREE plan of action.

Many Thanks,",Ignored,0,10
Google Cloud Certified Professional,"We're looking for a GCP CERTIFIED PROFESSIONAL to support our Dashboard Platform based on Media Connectors, Big Query and Looker Studio.
- 05 hour monthly minimum + on demand tasks
- It is mandatory to link individual certification to our domain (@predicta.net)
- GCP Certification must be active at least for the next 12 months
- Accepted Certifications:
- Professional Cloud Architect
- Professional Data Engineer
- Professional Cloud Security Engineer
- Professional Machine Learning Engineer
- Professional Cloud Database Engineer
- Professional Cloud Network Engineer
- Professional Cloud Developer
- Professional Cloud DevOps Engineer","I am a GCP certified professional data engineer with 5+ years of experience working with Python, BigQuery, SQL and Looker studio. I have worked for top companies such as Toast, Lyft, and Freestak (Nike, New Balance and UA data) and have extensive exposure with commercial-scale projects. I have attached some of my past projects and certification link below for verification purposes.

https://www.upwork.com/fl/usmanashraf678?p=1595718198020808704
https://www.upwork.com/fl/usmanashraf678?p=1693978841274896384
https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656
https://google.accredible.com/93316021-073a-44fb-a172-94dc7b7144d7

Let's connect and discuss further!

With Gratitude,",Ignored,0,50
Language Model Training and Prompt Engineering Specialist,"**Job Description: Language Model Training and Prompt Engineering Specialist**

We are in search of a Language Model Training and Prompt Engineering Specialist to join our team, focusing on the optimization and enhancement of large language models (LLMs) to interact seamlessly with our backend systems, particularly in the context of API integration and command execution. This role is pivotal in ensuring that our LLMs can efficiently process and understand requests related to a wide variety of third-party APIs and execute tasks through a comprehensive command library.

### Responsibilities:

- **LLM Fine-Tuning**: Implement strategies for fine-tuning large language models to improve their understanding and execution of tasks related to API interactions, ensuring high accuracy and reliability in command processing.
- **Prompt Engineering**: Develop and refine effective prompts that guide the LLMs in generating accurate and relevant responses, commands, and actions based on user requests and backend requirements.
- **Integration Workflow Optimization**: Work closely with the development team to create a seamless workflow between the LLMs and the backend systems, ensuring that the language models can effectively trigger and manage API calls.
- **Performance Monitoring and Evaluation**: Regularly assess the performance of the LLMs, identifying areas for improvement in understanding, response generation, and API interaction. Implement feedback loops for continuous model enhancement.
- **Documentation and Knowledge Sharing**: Maintain comprehensive documentation on prompt engineering techniques, model tuning processes, and integration practices. Share knowledge and best practices with the team to foster a collaborative and informed working environment.

### Required Skills and Qualifications:

- **Expertise in Natural Language Processing (NLP)**: Strong background in NLP, with a focus on large language models and their applications.
- **Experience with LLM Fine-Tuning**: Demonstrated experience in fine-tuning large language models for specific tasks or domains, including the use of techniques like transfer learning and few-shot learning.
- **Strong Skills in Prompt Engineering**: Ability to craft effective and efficient prompts that guide LLMs in generating desired outputs, with a keen understanding of how prompt design influences model responses.
- **Technical Proficiency**: Solid understanding of programming languages relevant to AI and machine learning, such as Python, and experience with AI/ML frameworks and tools.
- **Problem-Solving and Analytical Skills**: Excellent analytical and problem-solving abilities, with the capacity to identify challenges and opportunities for improvement in LLM performance and interaction.

### Nice-to-Have:

- **Background in API Integration**: Familiarity with API integration concepts and challenges, which would be beneficial for understanding the end-to-end workflow and the interaction between LLMs and backend systems.
- **Experience in an Agile Development Environment**: Comfortable working in fast-paced, Agile development teams, with a collaborative and adaptable approach to project work.

This position offers a unique opportunity to contribute to the cutting edge of LLM application and development, directly impacting the effectiveness and innovation of our platform. If you are passionate about the potential of large language models to transform interactions with technology and are skilled in the art and science of prompt engineering and model training, we would love to have you on our team.","I am a Language Model Training and Prompt Engineering Specialist having created a number of fine-tuned LLMs. I also have integrated 3rd party APIs in a number of projects as well.

Here are some of my relevant skills that I have used in my projects (can be seen under upwork job history):
LLM Fine-Tuning
Prompt Engineering
Integration Workflow Optimization
Language Model Training
API Integration
Language Model Training
Prompt Engineering
Technical Proficiency
Problem-Solving and Analytical Skills
third-party APIs

Let's connect and discuss!
",Viewed,0,51
Senior Software Developer Needed for AI Document Processing Tool Integration with Oracle Netsuite,"Title: Senior Software Developer Needed for AI Document Processing Tool Integration with Oracle Netsuite

Description:
We are seeking a highly skilled senior software developer to spearhead the development of an innovative AI tool designed to streamline document processing and integration with Oracle Netsuite. The primary objective of this project is to automate the extraction of information from various documents and seamlessly populate Oracle Netsuite forms with the extracted data.

Responsibilities:
Collaborate with our team to understand the requirements and objectives of the project.
Design and develop an AI-powered document processing tool capable of accurately extracting relevant information from diverse document types.
Integrate the developed tool with Oracle Netsuite to automate the population of forms with the extracted data.
Implement robust error handling and validation mechanisms to ensure the accuracy and reliability of data extraction and form population processes.
Optimize the performance and scalability of the solution to handle large volumes of documents efficiently.

Requirements:
Extensive experience in software development, with a focus on AI and Python.
Proficiency in AI technologies such as natural language processing (NLP) and machine learning for document analysis and data extraction.
Strong background in API integration, ideally with Oracle Netsuite or similar ERP systems.
Familiarity with document processing frameworks and libraries (e.g., SpaCy, Tesseract, NLTK).
Excellent problem-solving skills and attention to detail.
Effective communication skills and ability to collaborate with cross-functional teams.


Preferred Qualifications:
Prior experience developing AI-driven solutions for document automation or similar projects.
Knowledge of Oracle Netsuite APIs and customization capabilities a plus.
Experience with cloud platforms such as AWS or Azure for deploying and scaling AI applications.
If you are a talented software developer with a passion for leveraging AI to streamline business processes, we would love to hear from you. Please submit your proposal outlining your relevant experience and approach to this project. Include any relevant portfolio or past projects demonstrating your expertise in AI and software development.

Note: We are open to discussing project details and scope further during the interview process. Flexible working arrangements and competitive compensation are available for the right candidate. ","I am a senior software developer with experience in AI document processing. I have completed several projects in the AI Document Processing domain. Here are the skills that I used in these projects (listed under my upwork profile):

Oracle Netsuite APIs: https://www.upwork.com/freelancers/usmanashraf678?p=1757640024723824640
SpaCy, Tesseract, NLTK: https://www.upwork.com/freelancers/usmanashraf678?p=1693960848319623168
document analysis: https://www.upwork.com/fl/usmanashraf678?p=1693954539616645120
data extraction: https://www.upwork.com/freelancers/usmanashraf678?p=1595718198020808704
natural language processing (NLP): https://www.upwork.com/freelancers/usmanashraf678?p=1693960848319623168
AI Document Processing: https://www.upwork.com/jobs/~01b1736d88adee2468
Python: https://www.upwork.com/freelancers/usmanashraf678?p=1758076525091692544
Cloud services (AWS or Azure): https://www.upwork.com/freelancers/usmanashraf678?p=1594587629741510656

Let's conect and discuss.

Many Thanks,",Ignored,0,50
"Data Engineer: Expert in Data Scraping, Cleaning, Analysis, and Visualization","The primary responsibilities of this role include collecting and cleaning data, creating insightful visualizations and reports, and effectively communicating findings. The ideal candidate will possess exceptional attention to detail, robust analytical skills, and a high level of proficiency in data visualization tools. Proficiency in SQL and a proven track record of data storytelling are highly desirable.

We are seeking an individual proficient in Python and SQL, with exceptional research skills. The successful candidate will be tasked with conducting original research using publicly available data sources (e.g., data.gov, https://datasetsearch.research.google.com/, etc.).

Requirements:

- Excellent data interpretation and visualization skills.
- Ability to present complex data in a clear and concise manner.
- Proficiency in Python.
- Someone creative who can help us figure out what type of data
-Basic understanding of economics

This role is ideal for someone who is adept at leveraging publicly available data to generate original research and insights.

We would be using datasets like (to name a few):
https://fred.stlouisfed.org/
https://bls.gov/
https://www.bea.gov/

But we don't solely focus on economic data, as you will see from our attached samples.
Attached is an example of what we are looking for.

For example, let's say we are working on the Top 10 Best Cities for Working Remotely. We identify 5 metrics critical for evaluating these cities.

The 5 metrics are:
- Internet Quality
- Cost of Living
- Safety
- Healthcare Quality
- Community and Lifestyle

The methodology would be as follows:

- Data Collection: Gather the latest data for each metric from reliable sources for all cities under consideration.
- Normalization: Convert each metric to a common scale (e.g., 0 to 100) to ensure comparability.
- Weighted Scoring: Multiply each normalized metric score by its corresponding multiplying factor to get weighted scores.
-Add [ceutical], to your response to make sure you read the job post.
- Aggregate Score: Sum the weighted scores for each city to get an aggregate score reflecting its overall suitability for remote work.
- Ranking: Rank the cities based on their aggregate scores from highest to lowest to identify the top 10 best cities for working remotely.

A dataset that can be used to gather this data can be: https://cbb.census.gov/

The end result is attached so you can review it.","I am a GCP certified Professional Data Engineer with half a decade of experience in Data Scraping, Cleaning, Analysis, and Visualization. I am very proficient in Python and SQL. Moreover, I have worked on scraping a number of government databases. My last client https://www.withpavilion.com/ is a company that specializes in sharing government data and I did the scraping for them. I used Python with selenium, scrapy and beautiful soup to scrape the government websites such as https://www.bea.gov/.

I have worked as data analyst/engineer top companies such as Toast, AWS, Lyft and others. Here are some projects I have done along with the skills they used:
data interpretation and visualization: https://pos.toasttab.com/blog/on-the-line/nfl-cities-food-alcohol-trends (final article)
present complex data in a clear and concise manner: https://www.upwork.com/fl/usmanashraf678?p=1693978841274896384
data interpretation and visualization skills: https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656

Let's connect and discuss further!

Many Thanks,",Viewed,0,10
Data Engineer / Big Data Specialist,"About the Role:
As a Data Engineer / Big Data Specialist, you will be instrumental in tracking and analyzing user behavior in real time. Your expertise will enable us to understand user interactions, preferences, and feedback, helping us to continuously improve our platform and services.

Responsibilities:

Design and implement real-time data processing pipelines using technologies like Apache Kafka and Spark Streaming.
Manage the ingestion, processing, and analysis of user behavior data, including searches and messages.
Utilize analytics databases and tools to provide actionable insights on user behavior.
Ensure the highest standards of data privacy and security in handling user data.
Work closely with the development and product teams to translate data insights into actionable improvements.
Skills and Qualifications:

Proficiency in real-time data processing frameworks and tools.
Experience in setting up and managing complex data pipelines.
Knowledge of analytics and data visualization tools for real-time analytics.
Strong understanding of data privacy and security best practices.
Excellent analytical and problem-solving abilities.
Bachelor’s or Master’s degree in Data Science, Computer Science, or a related field.

TO APPLY:
Provide links to real world examples and describe your contribution to the projects.","I am a GCP certified Professional Data Engineer / Big Data Specialist with half a decade of experience in the field. During this tenure, I have created many applications that track and analyze user behavior in real time. I have implemented real-time data processing pipelines using Apache Kafka and Spark Streaming. Here are some of the relevant projects:

GCP + Data engineering + real-time processing: https://www.upwork.com/fl/usmanashraf678?p=1693978841274896384
AWS + analytics + data visualization: https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656

Let's connect and discuss this further.

Many Thanks,",Ignored,0,51
GA4 and GTM expert,"Looking for an experienced data analyst to carry a full audit of our GA4 and GTM

Title: Data Analyst Task: GA4 and GTM Audit

Tasks:

GA4 Audit:
- Review GA4 configuration settings thoroughly.
- Verify data collection methods and ensure accuracy.
- Identify any discrepancies between GA4 data and internal records.
- Assess conversion tracking and goal configurations.

GTM Evaluation:
- Scrutinize GTM tag implementations across website pages.
- Confirm proper deployment and functionality of tags.
- Check for duplicate or redundant tags and clean up if necessary.
- Validate event tracking and custom variables.

Data Consistency Check:
- Compare data from GA4 with other relevant sources for consistency.
- Investigate any irregularities or unexpected variations in data.
- Determine the root causes of discrepancies and propose solutions.

Tag Error Identification:
- Identify any errors or issues in tag configurations.
- Debug and troubleshoot tag firing problems.
- Ensure proper tagging for tracking key user interactions and events.

Documentation and Reporting:
- Document all findings, observations, and recommendations.
- Prepare a detailed report outlining audit results and actionable insights.


This task aims to ensure the accuracy, reliability, and integrity of our data tracking and reporting systems. By conducting a comprehensive audit of GA4 and GTM, we seek to optimize our data infrastructure and enhance our decision-making capabilities.","I am an expert Data Analyst with 5+ years of experience in the field and have set up and audited GA4 and GTM for several clients. In these engagements, I have created reports while ensuring the accuracy, reliability, and integrity of the data infrastructure for better decision making. I have worked with all the important metrics, like user source/medium, page views, screen views, demographics, conversion tracking, and other important custom events such as button clicks and form submissions, and you can check more details about one of my past projects at https://www.upwork.com/freelancers/usmanashraf678?p=1758073102791856128

Let's connect for a quick session to discuss the specifics of the job and I'd love to devise and share my strategy (plan of action) for FREE.

Many Thanks,",Ignored,0,50
Data Engineers Wanted for AI Startup in the USA - Potential Full-Time Opportunity,"**You must enclose your portfolio and Github with data engineering experiences to be considered for this job**
**We are hiring 1 junior & 1 medium level people for 3 months**

Our AI startup, situated in the heart of the USA's tech innovation scene, is pushing the boundaries of what's possible with data-driven technologies. As we scale our operations, we're in search of skilled Data Engineers who are passionate about building scalable data infrastructure and pipelines that will fuel our AI solutions. This role not only offers the chance to work on cutting-edge projects but also holds the potential for transitioning into a full-time position within our rapidly growing company.

Responsibilities:

* Design, construct, install, test, and maintain highly scalable data management systems.
* Ensure systems meet business requirements and industry practices for data integrity, reliability, and performance.
* Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
* Integrate new data management technologies and software engineering tools into existing structures.
* Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
* Collaborate with data architects, modelers, and IT team members on project goals.


Requirements:

* Proven experience as a Data Engineer, Software Developer, or similar role with a focus on data engineering.
* Strong programming skills, preferably in Python, Scala, or Java.
Experience with big data tools (Hadoop, Spark, Kafka, etc.) and data pipelines.
* Familiarity with data warehousing solutions and proficient understanding of distributed computing principles.
* The ability to manage and communicate data warehouse plans to internal clients.
* Experience with machine learning algorithms and data science techniques is a plus.
* Bachelor's degree in Computer Science, Engineering, or a related field, or equivalent work experience.


To Apply:
Please ensure your application includes the following:

* Your updated CV, detailing relevant experience and projects.
* A portfolio or collection of your work, demonstrating your skills and accomplishments in data engineering.
* A link to your GitHub profile or other repositories where your work can be reviewed. ","I am a GCP certified professional data engineer with half a decade of experience in the field. As a data engineer, I've created scalable data infrastructure and pipelines using Python, SQL, dbt, Hadoop and Spark. I have also created data ingestion scripts that get the data from various APIs and websites. I've extensively worked with data warehouses such as Redshify, Big query and snowflake.

I am attaching my resume. You can check out my portfolio projects under my upwork portfolio. I have delivered over a hundred projects. Here's my github: https://github.com/ahmad-amin-farooq.

Let's connect and discuss this further!

Many Thanks,",Ignored,0,51
Senior Data Engineer,"Role Overview:
As a Data Engineer at Onyxia, you will be responsible for spearheading the development of distributed, real-time, high-volume data pipelines. You will collaborate closely with our team to enable high-scale Data Science initiatives, leveraging cutting-edge open-source technologies such as AWS Redshift, Prefect, Airbyte, DBT, and more. This role offers the opportunity to work across the entire stack, utilizing Python programming language. You will have the chance to own significant parts of our service, driving impact and growth within the company.

About Onyxia:
Onyxia was founded in 2022 with the mission to set a new standard in cybersecurity performance management. The Onyxia platform empowers CISOs with a holistic view of their cybersecurity programs and delivers actionable insights for high-performing cyber defense strategies that align with business objectives. Recognized as a ‘Company to Watch’ by Tech:NYC, Onyxia helps security leaders focus on what matters, maximize their security solutions and make smarter and more efficient decisions.

Responsibilities:
- Design and build distributed, real-time, high-volume data pipelines.
- Collaborate with the team to enable high-scale Data Science initiatives.
- Utilize open-source technologies such as AWS Redshift, Prefect, Airbyte, DBT, and more.
- Solve complex problems in a collaborative and efficient manner.
- Take ownership of meaningful parts of our service and drive impact within the company
Requirements
- BS/MS in a scientific field or equivalent experience.
- 4+ years of experience writing in Python.
- Experience with SQL.
- Experience building and operating large data sets, construct ETL pipelines and building the workflows.
- ETL tools building experience.
- Strong knowledge in data manipulation and analysis tools, such as Pandas.
- Experience with warehouses, such as: AWS Redshift, BigQuery.
- Passion for wrangling large volumes of data and exploring new datasets.
- Commitment to code simplicity and performance optimization.
- Desire to work in a fast-paced, high-growth startup environment.
- Design, develop, test, deploy, maintain and improve the software with a strong
focus on customer-facing features.

If you are a passionate Data Engineer looking to join an innovative startup where you can make a real impact, we would love to hear from you. Join us in shaping the future of Onyxia!!","I am a GCP certified professional data engineer having served several top clients and early-stage startups, like Lyft, Toast, and Freestak (Nike, NB, UA data), as a data engineer/analyst. I have been working for 5+ years as a data engineer and have advanced skills in Python, SQL, dbt, and Spark. I have built real-time and high-volume data pipelines and datawarehouses for these companies and would love to do the same for your startup.

I am also well-equipped with differnet programming languages, like Node.js and Javascript, and AWS cloud technologies, such as S3, Glue, Athena, and Redshift. I am highlighting some of my relevant projects (documented on Upwork) to verify my expertise and you can also check them at the following links.

https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656
https://www.upwork.com/freelancers/usmanashraf678?p=1758071509031018496
https://www.upwork.com/fl/usmanashraf678?p=1693978841274896384

Let's connect for a quick session to discuss the specifics of the job and I would love to share a FREE plan of action (or SOW) for developing the application roadmap.

Many Thanks!",Ignored,0,51
Analytics Engineer,"We need an analytics engineer to support reporting and data analytics for a company that sells software and services using a subscription model. We use dbt for data modeling and R for additional analytics. This position will be focused on handling reports for marketing, both maintaining existing reporting models and supporting new areas of interest

Responsibilities:
* Develop and maintain star schema-based data models that facilitate efficient data analysis and reporting.
* Ensure data accuracy, integrity, and consistency within the data models.
* Design and implement test queries to ensure the accuracy and reliability of data models and reports.
* Perform data validation and verification to maintain data quality standards.
* Work closely with stakeholders to understand their requirements and data needs.
* Optimize database performance to improve query response times and overall data processing efficiency.

Qualifications:
* Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
* Proven experience as an Analytics Engineer or similar role, with expertise in data modeling and data transformation.
* Proficiency in SQL, dbt, and R for data analysis and reporting.
* Familiarity with star schema methodology and data warehousing concepts.
* Ability to work collaboratively in a fast-paced, team-oriented environment.
* Excellent communication skills, both verbal and written, to effectively convey technical concepts to non-technical stakeholders. ","I have looked into RealDefense's top brands and assuming that this job is pertaining to iolo's data reporting and analytics. The following about my profile will make your decision making process a lot easier:

1. I have half a decade of experience in creating data analytics/reporting for marketing data
2. I have worked as a analytics engineer for enterprise clients companies such as AWS, Toast, Lyft
3. I am an expert in the technologies involved: SQL, dbt and R
4. I have created and maintained data modelling and transformations for a SMEs as well. One of the clients that I work for is an influencer marketplace. They have a ton of marketing data as well. I think there is massive coherence with this role. Their whole pipeline is created/maintained by me in dbt (SQL).
5. I have excellent communication skills having lived and studied in the US. I can effectively communicate with technical and non technical stakeholders.

I am attaching some of my portfolio porjects.

Let's connect and discuss this further, Steve. I would love to be a part of RealDefense.

Many Thanks,",Ignored,0,50
PowerBI and SnowFlake Dashboard Creation and Data Pipeline Help,"We are seeking an experienced dataviz pro with expertise in Snowflake, data visualization using Power BI, and data pipelines to take over an ongoing project. The ideal candidate will have a strong background in Snowflake and be able to hit the ground running.

Key Responsibilities:

Implement and manage data ingestion processes from various sources into Snowflake
Design and develop a data model in Snowflake to support reporting requirements
Create and optimize Power BI dashboards and reports based on the Snowflake data model
Collaborate with stakeholders to gather requirements and ensure project success
Required Skills:

Extensive experience with Snowflake (data warehousing, data modeling, performance optimization)
Proficiency in data visualization using Power BI
Familiarity with data pipeline tools (e.g., Fivetran)
Strong communication and collaboration skills
Ability to work independently and manage project deliverables
Project Background:
The Daily Summary 2.0 project aims to replace an existing Excel-based reporting system with a Snowflake-powered solution. The contractor will be responsible for completing the unfinished work, which includes data ingestion, data modeling, and Power BI dashboard creation.","I know it's not the best situation having to switch resources in mid project. However, don't worry! I am here to take Daily Summary 2.0 to the finish line in a timely and efficient manner.

About me:
I am a GCP certified professional data engineer with half a decade of experience in creating dashboards based on power BI and snowflake. I have created a data pipelines based on snowflake previously. Here's a project that used snowflake for the data warehouse:

Snowflake + Fivetran + dbt (for marketing data reporting): https://www.upwork.com/fl/usmanashraf678?p=1594587629741510656

Let's connect and I'd love to share my plan of action for this!

Many Thanks,",Ignored,0,50
Python Data Engineer,"As a Data Engineer at Polynom, you will play a crucial role in designing, developing, and implementing robust data pipelines and APIs. More information can be found in the attached document.
","I am a GCP certified professional data engineer with half a decade of experience in creating data pipelines. I understand that you are looking for a lower experience person. But, I think you'll benefit greatly from my experience. Also, I can provide you a reasonable hourly rate for the long term. I have worked on all relevant technologies such as:

Python
FastAPI
SQLAlchemy
Pandas

Let's connect and discuss!

Ps. Please check my attached resume.

Many Thanks,",Ignored,0,10
Expert Data Analyst Needed for Amazon Seller API Integration and BI Dashboard Creation,"We are an Amazon Agency managing multiple brands and seller accounts. We are seeking an experienced Data Analyst who can effectively integrate Amazon Seller API data across various accounts into a centralized SQL database and create dynamic BI dashboards using PowerBI. This role requires someone with strong technical skills in API integration, database management, and business intelligence reporting.

We can set a price for the setup and a monthly compensation for maintenance.

Responsibilities:
- Develop and maintain a robust data pipeline that fetches data regularly from the Amazon Seller API.
- Clean, transform, and store data in a centralized SQL database.
- Design and implement PowerBI dashboards that display key performance indicators and financial metrics across all brands and accounts.
- Ensure data accuracy and integrity in reporting.
- Provide monthly updates and optimizations based on stakeholder feedback.

Required Skills:
- Strong experience with Python programming for API interactions and data processing.
- Proficient in SQL for managing large datasets and performing complex queries.
- Proven track record of designing and implementing interactive dashboards with PowerBI.
- Experience with the Amazon Seller API is highly preferred.
- Excellent analytical, problem-solving, and communication skills.

Project Deliverables:
- A fully functional data pipeline that integrates with multiple Amazon Seller accounts.
- A SQL database schema designed for optimal data analysis and reporting.
- Interactive PowerBI dashboards that can be accessed by stakeholders to make informed business decisions.

Application Requirements:
- Please provide examples of similar projects you have completed, specifically with API integrations and BI dashboard development.
- Include a brief proposal on how you would approach this project, including tools and methodologies.
Indicate your availability and estimated time frame to complete this project.
- We are looking for a detail-oriented and reliable freelancer to join our team on this crucial project. If you are ready to help us transform our data analysis capabilities, we would love to hear from you! ","I have worked on the Amazon Seller and Vendor Central APIs. Happy to connect and collaborate on this.

You can check some of the relevant projects here: https://www.upwork.com/fl/usmanashraf678?p=1693976404040691712",Ignored,0,50
"AI Expert to Create AI Workflow with OpenAI, GPT and RAG","We are seeking a skilled AI expert to design and implement an advanced AI workflow tailored for our growing social media data needs. We need someone with experience with cutting-edge technologies including GPT, Large Language Models (LLM), and Retrieval-Augmented Generation (RAG) to enhance our data with automated sentiment analysis, topic detection, and intelligent labeling.

We have lots of human verified data (nearly 500k) and we get millions of data per month so we need assistance to manage these data and enrich it.","I am a AI expert having created dozens of AI workflows for my clients that streamline their day to day tasks. I have worked with GPT APIs to do automated sentiment analysis and topic detection using Large Language Models (LLM), and Retrieval-Augmented Generation (RAG). I have also used their Vision API to extract information from Instagram posts. I am curious what data form are you going to be dealing within social media: text or images. By the way, I have extensively worked with both.

For the topic extraction and sentiment analysis, I worked with cloutly (https://cloutly.com/) to extract the sentiment of the reviews gathered by them and also to identify what topic is being discussed in a review. I think this is very similar to what you are looking for.

Let's connect and discuss this further.

Many Thanks,",Viewed,0,15
"Microsoft SQL Server, Azure Data Factory and Azure Data Like Project - NEED EXPERTISE!","We have a number of systems that all generate data. Today, we create reports by directly connecting to these systems or having some systems put data in snowflake or equilvalent.

Instead, we want to rethink our entire data strategy and all reporting. We want to leverage Azure Data Lake and Azure Data Factory to integrate to all of our apps, via API, to pull and populate a set of standardized tables in the lake. Orders, Customers, Products, etc.

Your job is to look at all of the systems we need to pull data from (QuickBooks, Looker Studio, Google Big Query, Custom Applications, Commercial Systems like StoreForce, Tulip, Magento. Then pull it all into the data lake working with the existing data team.

Once completed, we will need to rewrite most of our existing reports . Our reports today are in PowerBI or Excel spreadsheet dumps. You will work to rebuild them one by one with the existing data team until they are all complete.","I understand that you are looking to create a central data lake that would pull data from sources such as QuickBooks, Looker Studio, Google Big Query, StoreForce, etc. I have worked with APIs of most of these services. I am curious what models you have within these sources?

About me:
I am GCP certified professional Data Engineer with half a decade of experience in creating data pipelines in both Azure ecosystem and otherwise. I am currently working on a project that creates a data lake within the Azure cloud. Here's that job on upwork: https://www.upwork.com/jobs/~01eee1b7d294ea4c03. You can find it under my current projects as well.

I understand that you want to rethink your data strategy and reporting. I am curious what are your key business goals around creating the new reports.

Let's connect and discuss!

Many Thanks,",Ignored,0,50
Using ChatGPT to support medical diagnostic reasoning,"We are studying how physicians can be supported by ChatGPT in finding an accurate diagnosis. To that end, we present clinical vignettes (i.e., descriptions of a patient's findings) to physicians by using web-based surveys, such as Qualtrics. But any other web-based interactive survey will do. The input to ChatGPT would be a diagnosis provided by a physician and it should return an answer to the question: “What are the most relevant findings (history, physical examination, and laboratory findings) associated with ……. (here to fill in the physician’s diagnosis)
Sometimes, the physician receives more than one opportunity to produce a diagnosis, to be submitted to ChatGPT.
We have produced a mock (example) survey in which the interaction with ChatGPT is described. See the document attached. The relevant input and responses of ChatGPT are to be found on pages 10 and 15.
We need your help in integrating ChatGPT’s API in a web-based survey instrument, preferably Qualtrics, but if that is not possible then we need your help in finding a survey type that fits our goals.","I have looked at your requirements and understand that you're looking to extract the most relevant finding e.g. history, physical examination, and laboratory findings from a doctor's report in a diagnosis using GPT. I understand that you want to integrate this into Qualtrics. As a GPT expert, I have integrated GPT APIs for various different projects e.g.

1. extraction of information from images
2, extraction of sentiment and topics from online reviews

Please check out my jobs under progress for more relevant projects.

I would love to connect and discuss this project further with you, Henk.

Let's chat!

Many Thanks,",Viewed,0,15
Python engineer with timeseries data experience,"For the development of a SaaS platform for monitoring and optimization of Solar PV plants, we are inviting a Python engineer with background in time series, data engineering and backend application development to join.

The engineer will be creating a new platform for data storage, processing and forecasting using the latest technologies from the ground-up.

We are flexible about the specific technologies and we are inviting the engineers for an open discussion.

In particular, we prefer someone that has experience in some of the bellow:

- FastAPI, Flask, Django
- Celery
- Pandas dataframes, numpy
- Grafana
- Google Cloud Platform, specifically Cloud Run or App Engine
- Postgres, BigQuery and Bigtable
- Terraform
","I understand that you are looking to build a SaaS platform for monitoring and optimization of Solar PV plants from the ground up. As an expert in Python and data engineering with half a decade of experience, I think I'll be the ideal fit for this role. I have created analytics platforms that have real-time and batched reporting previously such as Gung Ho Freestak Ltd (https://www.gunghoco.com/freestak) and Loop AI (https://loopkitchen.xyz/)

I also have expertise in the following:
- FastAPI, Flask, Django
- Celery
- Pandas dataframes, numpy
- Grafana
- Google Cloud Platform. I am GCP certified professional data engineer so you'd not have to worry about GCP technology
- Postgres, BigQuery and Bigtable

I am willing and available to have a discussion around how to build this platform. Let's connect and discuss!

Many Thanks,",Ignored,0,50
"Data Engineer (BigQuery, SQL)","🤠 Howdy, we’re Frontiers Market! We're an exciting new startup in Austin that’s all about helping ranchers and farmers streamline their operations. We're in search of a talented Data Engineer to harness the power of BigQuery and SQL to enhance our data-driven decisions and marketplace insights.

🎯 What You'll Be Doin'

- Design, develop, and maintain scalable data pipelines using BigQuery and SQL.
- Optimize data architecture to improve performance and efficiency in our marketplace.
- Implement machine learning models to predict trends and automate decision-making processes.
- Collaborate with cross-functional teams to integrate systems and streamline data flow.
- Provide insights and recommendations to enhance marketplace strategies and operations.

🛠 Requirements

- Proficiency in SQL and experience with BigQuery or similar data warehouse technologies.
- Strong background in developing and optimizing data pipelines, architectures, and data sets.
- Experience in implementing AI and machine learning models in a business environment.
- Ability to work independently and collaboratively.
Strong problem-solving skills.
- Good written and verbal communication skills.

🎉 Perks

- Competitive salary and equity.
- Flexible working hours.
- Opportunity to work on a product that's revolutionizing a 🐮 industry.

🤝 Let's Get to Work

If you're lookin' for a role where you can make a real impact and help shape the future of an industry that's the backbone of America, then saddle up and join the Frontiers Market team. We're not just building a product; we're building a community, and we want you to be a part of it. We can't wait to see what you'll bring to our round-up. 🐮🎉","I just checked your website and really enjoyed looking at some pretty cow/bull picture, haha.

Anyways, I understand that you're looking to enhance your data-driven decisions and marketplace insights using Big Query and SQL. I am curious what data points do you want to analyze. Is this engagement data from the marketplace listings?

About me: I am a GCP Certified Professional Data engineer with 5+ years of experience in developing and optimizing data pipelines in Big Query using SQL and presenting them using visualization tools such as Looker etc. I am also well-versed with Big Query ML from my experience as a data engineer. From this, we can produce some amazing insights for your business. I have done this for enterprise clients such as AWS, Toast Inc and Lyft in the past.

I'll highlight my relevant projects with the proposal. Please do check them out.

Let's connect and have a discussion on what's are some interesting insights we can pull.

Many Thanks,",Ignored,0,51
